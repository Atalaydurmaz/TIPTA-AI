# -*- coding: utf-8 -*-
"""
REINFORCE (Politika GradyanÄ±) algoritmasÄ± ile CartPole-v1 ortamÄ±nÄ± eÄŸitir.
GÃ–RSELLEÅTÄ°RME: OrtamÄ±n penceresini aÃ§arak arabanÄ±n hareketlerini izler.
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import backend as K
import gymnasium as gym
import matplotlib.pyplot as plt

# ---------------- 1ï¸âƒ£ Ortam ve Model TanÄ±mlamalarÄ± ----------------

# Gymnasium OrtamÄ±nÄ± GÃ–RÃœNTÃœLEME MODU (render_mode="human") ile baÅŸlatma
# Bu, eÄŸitimi izlerken simÃ¼lasyon penceresinin aÃ§Ä±lmasÄ±nÄ± saÄŸlar.
env = gym.make("CartPole-v1", render_mode="human")
state_size = env.observation_space.shape[0] # 4 boyutlu durum
action_size = env.action_space.n            # 2 eylem
LEARNING_RATE = 0.001
GAMMA = 0.99                                # Monte Carlo Getirisinde Ã¶nemli

# ---------------- 2ï¸âƒ£ Politika AÄŸÄ± (Policy Network) ----------------
def build_policy_model():
    model = Sequential()
    model.add(Dense(32, activation='relu', input_shape=(state_size,)))
    model.add(Dense(32, activation='relu'))
    # Ã‡Ä±ktÄ±: Her eylem iÃ§in olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± (Softmax)
    model.add(Dense(action_size, activation='softmax'))
    return model

policy_model = build_policy_model()

# ---------------- 3ï¸âƒ£ REINFORCE KayÄ±p Fonksiyonu ----------------

def reinforce_loss(y_true, y_pred):
    # y_true (GerÃ§ek DeÄŸerler): [seÃ§ilen_eylem_indeksi, kÃ¼mÃ¼latif_getiri (G)]
    
    reward = y_true[:, 1] #Her Ã¶rneÄŸin Ã¶dÃ¼lÃ¼nÃ¼ alÄ±r.
    action_index = tf.cast(y_true[:, 0], tf.int32) #Her Ã¶rnekte seÃ§ilen eylem indeksini alÄ±r.
    
    # SeÃ§ilen eylemin olasÄ±lÄ±ÄŸÄ±nÄ± bul
    batch_indices = tf.range(tf.shape(y_pred)[0])
    indices = tf.stack([batch_indices, action_index], axis=1)
    probabilities = tf.gather_nd(y_pred, indices)
    
    # REINFORCE KayÄ±p: -log(pi(a|s)) * G
    log_probabilities = K.log(probabilities + K.epsilon())
    loss = - log_probabilities * reward
    
    return K.mean(loss)

policy_model.compile(loss=reinforce_loss, optimizer=Adam(learning_rate=LEARNING_RATE))

# ---------------- 4ï¸âƒ£ KÃ¼mÃ¼latif Getiri Hesaplama Fonksiyonu ----------------
# Gt = r_t + gamma * r_{t+1} + gamma^2 * r_{t+2} + ...
def compute_returns(rewards, gamma):
    returns = []
    G = 0
    # Ã–dÃ¼lleri sondan baÅŸa doÄŸru toplayarak Getiriyi hesapla
    for r in rewards[::-1]:
        G = r + gamma * G
        returns.insert(0, G)
    return np.array(returns)

# ---------------- 5ï¸âƒ£ EÄŸitim DÃ¶ngÃ¼sÃ¼ ----------------
N_EPISODES = 150 # GÃ¶rselleÅŸtirdiÄŸimiz iÃ§in daha kÄ±sa tuttuk.
MAX_STEPS = 500  # Bir epizotun maksimum uzunluÄŸu
score_history = []

print("\nğŸ¯ REINFORCE (CartPole) eÄŸitimi baÅŸlatÄ±ldÄ± ve gÃ¶rselleÅŸtiriliyor...")
print("EÄŸitim sÄ±rasÄ±nda CartPole penceresini gÃ¶receksiniz. (Kodu durdurmak iÃ§in kapatabilirsiniz)")

for ep in range(1, N_EPISODES + 1):
    
    # Her epizot iÃ§in verileri sÄ±fÄ±rla
    states, actions, rewards = [], [], []
    
    # 1. Epizotu Ã‡alÄ±ÅŸtÄ±r ve Deneyimi Topla
    state, _ = env.reset()
    done = False
    
    while not done and len(states) < MAX_STEPS:
        
        # State'i aÄŸa uygun boyuta getir: (1, 4)
        state_input = state.reshape(1, state_size)
        
        # Politika AÄŸÄ±nÄ± Kullanarak Eylem OlasÄ±lÄ±klarÄ±nÄ± Tahmin Et
        action_probs = policy_model.predict(state_input, verbose=0)[0]
        
        # OlasÄ±lÄ±klara GÃ¶re Rastgele Eylem SeÃ§
        action_idx = np.random.choice(action_size, p=action_probs)
        
        # Eylemi GerÃ§ekleÅŸtir
        next_state, reward, done, truncated, _ = env.step(action_idx)
        
        # ğŸŒŸ GÃ–RSELLEÅTÄ°RME KOMUTU ğŸŒŸ
        # Bu satÄ±r, her adÄ±mda CartPole simÃ¼lasyonunu gÃ¼ncelleyip ekranda gÃ¶sterir.
        env.render() 
        
        # Deneyimi Kaydet
        states.append(state)
        actions.append(action_idx)
        rewards.append(reward)
        state = next_state
    
    # Toplam Skoru Kaydet
    total_reward = sum(rewards)
    score_history.append(total_reward)
    
    # 2. KÃ¼mÃ¼latif Getiriyi Hesapla
    returns = compute_returns(rewards, GAMMA)
    
    # 3. EÄŸitimi HazÄ±rla ve AÄŸÄ± GÃ¼ncelle
    target_output = np.stack([actions, returns], axis=1)
    
    policy_model.fit(
        np.array(states),
        target_output,
        epochs=1,
        verbose=0,
        shuffle=False
    )
    
    # Konsol Ã‡Ä±ktÄ±sÄ±
    if ep % 50 == 0:
        print(f"Epizot: {ep}/{N_EPISODES}, Son 50 Ort. Puan: {np.mean(score_history[-50:]):.2f}")

# ---------------- 6ï¸âƒ£ OrtamÄ± Kapat ----------------
# EÄŸitim bittiÄŸinde aÃ§Ä±lan pencereyi kapatÄ±r.
env.close()

# ---------------- 7ï¸âƒ£ Grafik ----------------
window_size = 50
plt.figure(figsize=(12, 6))
plt.plot(np.convolve(score_history, np.ones(window_size)/window_size, mode='valid'))
plt.title("REINFORCE (CartPole-v1): 50-Epizotluk Hareketli Ortalama Puan")
plt.xlabel("Epizot")
plt.ylabel("Ortalama Puan (DireÄŸi Dik Tutma SÃ¼resi)")
plt.axhline(y=475, color='r', linestyle='--', label='Ã‡Ã¶zÃ¼lmÃ¼ÅŸ EÅŸik (475)')
plt.legend()
plt.show()