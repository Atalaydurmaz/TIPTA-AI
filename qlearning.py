# -*- coding: utf-8 -*-
"""
Created on Tue Oct 28 20:18:47 2025

@author: Fatma
"""

# -*- coding: utf-8 -*-

import pandas as pd
import numpy as np
import random
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import KBinsDiscretizer

# ---------------- 1ï¸âƒ£ Veri YÃ¼kleme ----------------
print("ğŸ“¥ UCI COVID-19 verisi yÃ¼kleniyor...")
# LÃ¼tfen KENDÄ° DOSYA YOLUNUZU BURAYA YAZIN
file_path = r"C://Users//Fatma//Desktop//V4//covid_50.csv" 
try:
    df = pd.read_csv(file_path)
    print("âœ… Veri baÅŸarÄ±yla yÃ¼klendi!")
except FileNotFoundError:
    print("HATA: Dosya yolu bulunamadÄ±. LÃ¼tfen dosya yolunu kontrol edin.")
    exit()

# ---------------- 2ï¸âƒ£ Ã–n iÅŸleme ----------------
target_col = 'COVID-19'

# Kategorikleri sayÄ±ya Ã§evir
for c in df.columns:
    if df[c].dtype == object:
        df[c] = LabelEncoder().fit_transform(df[c].astype(str))#METÄ°N tabanlÄ± tÃ¼m 
        #sÃ¼tunlarÄ± 0,1,2 gibi tamsayÄ± deÄŸerlerine Ã§evirir.

X = df.drop(columns=[target_col])
y = df[target_col]

# Eksik veri doldur
X = X.fillna(0)#Eksik deÄŸerler 0 ile doldurulur
y = y.fillna(0)

# ---------------- 3ï¸âƒ£ Diskritizasyon ve State Temsili ----------------
N_BINS = 2 #Her sayÄ±sal Ã¶zelliÄŸi iki kateoriye ayÄ±rmayÄ± seÃ§er
kbd = KBinsDiscretizer(n_bins=N_BINS, encode='ordinal', strategy='uniform')#Veriyi
#Ã¶ÄŸrenilen NBINS sayÄ±sÄ±na gÃ¶re bÃ¶lÃ¼yor
X_disc_all = kbd.fit_transform(X)#tÃ¼m sayÄ±sal Ã¶zellikleri 0 veya 1 gibi ayrÄ±k deÄŸerlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r

# Diskritize edilmiÅŸ veriyi kullanacaÄŸÄ±z
X_train_disc, X_test_disc, y_train, y_test = train_test_split(
    X_disc_all, y, test_size=0.2, random_state=42
)

# DurumlarÄ± temsil eden Ã¶zellik sayÄ±sÄ±
n_features = X_train_disc.shape[1]
actions = [0, 1, 2]  # 0: no action, 1: test, 2: treat
n_actions = len(actions)

print(f"\nModel Girdi Boyutu (Ã–zellik SayÄ±sÄ±): {n_features}")
print(f"Eylem sayÄ±sÄ±: {n_actions}")

# ---------------- 4ï¸âƒ£ Ã–dÃ¼l fonksiyonu ----------------
def reward_fn(action, true_label):
    if action == 0:  # hiÃ§bir ÅŸey yapma
        # true=0 ise Ã¶dÃ¼l 0, true=1 ise ceza -1
        return 0 if true_label == 0 else -1 
    if action == 1:  # test et
        # true=1 ise Ã¶dÃ¼l 1, true=0 ise test maliyeti -0.2
        return 1 if true_label == 1 else -0.2
    if action == 2:  # tedavi et
        # true=1 ise Ã¶dÃ¼l 1, true=0 ise yÃ¼ksek ceza -1 (yanlÄ±ÅŸ tedavi)
        return 1 if true_label == 1 else -1
    return 0

# ---------------- 5ï¸âƒ£ Q-TABLOSU (DQN Yerine) ----------------
# DurumlarÄ±n HASH'lenmesi: Q-tablosu iÃ§in her ayrÄ±k durumun benzersiz bir indeksini oluÅŸtururuz.
# Bu, (0, 1, 0, 1, ...) gibi ayrÄ±k Ã¶zellikleri tek bir tam sayÄ±ya Ã§evirir.
def state_to_index(s_disc):
    # s_disc: (n_features,) boyutunda numpy dizisi (Ã¶rneÄŸin: [0. 1. 0. 1. ...])
    s_tuple = tuple(s_disc.astype(int))
    return hash(s_tuple)
#Ä°ÅŸleyiÅŸ AdÄ±mlarÄ±
#GiriÅŸ: Fonksiyon, s_disc adÄ± verilen, ayrÄ±klaÅŸtÄ±rÄ±lmÄ±ÅŸ (discrete) durumu temsil eden bir NumPy dizisini alÄ±r.
# (Ã–rnek: [0. 1. 0. 1. ...]) TamsayÄ±ya DÃ¶nÃ¼ÅŸtÃ¼rme:s_disc.astype(int): Dizideki tÃ¼m kayan nokta (float) deÄŸerleri, tamsayÄ±lara dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r 
#Bu, tutarlÄ± bir hash deÄŸeri Ã¼retmek iÃ§in Ã¶nemlidir.Tuple'a DÃ¶nÃ¼ÅŸtÃ¼rme:tuple(...): TamsayÄ±lardan oluÅŸan NumPy dizisi, tuple (demet) veri yapÄ±sÄ±na Ã§evrilir.
#Neden Tuple? Python'da yalnÄ±zca deÄŸiÅŸmez (immutable) nesneler hash'lenebilir ve sÃ¶zlÃ¼k anahtarÄ± olarak kullanÄ±labilir. 
#NumPy dizileri ve listeler deÄŸiÅŸken (mutable) olduÄŸu iÃ§in doÄŸrudan anahtar olamazlar; ancak tuple'lar deÄŸiÅŸmezdir.
#Hashleme:hash(s_tuple): Elde edilen tuple, Python'Ä±n yerleÅŸik hash() fonksiyonu kullanÄ±larak tek, benzersiz bir tamsayÄ±ya (integer) dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r.
# PekiÅŸtirmeli Ã–ÄŸrenme'deki RolÃ¼Bu hash deÄŸeri, daha sonra Q-Tablosu'nda o duruma karÅŸÄ±lÄ±k gelen Q-deÄŸerlerini depolamak iÃ§in anahtar (key) olarak kullanÄ±lÄ±r. 
#Bu sayede, ajan o durumu tekrar ziyaret ettiÄŸinde, hÄ±zlÄ±ca ilgili Q-deÄŸerlerine eriÅŸebilir.

# TÃ¼m veri setindeki benzersiz durumlarÄ± bulalÄ±m
unique_states = {state_to_index(s) for s in X_train_disc}
#Bu satÄ±r, eÄŸitim veri setinde (training set) bulunan tÃ¼m benzersiz durumlarÄ± (states) toplar
# ve her bir durumu benzersiz bir anahtara (hash) dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.

Q_table = {state_hash: np.zeros(n_actions) for state_hash in unique_states}
#Bu satÄ±r, unique_states kÃ¼mesindeki her bir benzersiz durum anahtarÄ± iÃ§in Q-Tablosu'nu oluÅŸturur 
#ve baÅŸlangÄ±Ã§ Q-deÄŸerlerini sÄ±fÄ±r olarak ayarlar.

print(f"Q-Tablosu Boyutu (Benzersiz Durum SayÄ±sÄ±): {len(Q_table)}")

# ---------------- 6ï¸âƒ£ Q-Learning Parametreleri ----------------
ALPHA = 0.5  # Ã–ÄŸrenme OranÄ± (ALPHA) #AmaÃ§: AjanÄ±n yeni bilgiyi ne kadar ciddiye alacaÄŸÄ±nÄ± belirler.
GAMMA = 0.9  # Ä°ndirgeme FaktÃ¶rÃ¼ (Gelecekteki Ã¶dÃ¼llerin ne kadar Ã¶nemli olduÄŸunu belirler.)
EPS_START = 1.0 #Ã–ÄŸrenmenin baÅŸlangÄ±cÄ±nda keÅŸif yapma olasÄ±lÄ±ÄŸÄ±. Genellikle %100 (1.0) baÅŸlar, yani ajan en baÅŸta rastgele davranÄ±r.
EPS_END = 0.01 #Ã–ÄŸrenmenin sonunda keÅŸif yapma olasÄ±lÄ±ÄŸÄ±nÄ±n dÃ¼ÅŸeceÄŸi minimum deÄŸer.
EPS_DECAY = 0.99977 #eps deÄŸerinin her adÄ±mda ne kadar azaltÄ±lacaÄŸÄ±nÄ± (Ã§arpÄ±lacaÄŸÄ±nÄ±) belirler.
N_EPISODES = 20000  #AjanÄ±n toplamda kaÃ§ farklÄ± "oyun" veya "deneme" dÃ¶ngÃ¼sÃ¼ (epizot) yapacaÄŸÄ±nÄ± belirler.

eps = EPS_START
rewards_history = []
print("\nğŸ¯ Q-Learning baÅŸlatÄ±ldÄ±...")

# ---------------- 7ï¸âƒ£ EÄŸitim DÃ¶ngÃ¼sÃ¼ (Q-Learning MantÄ±ÄŸÄ±) ----------------
for ep in range(1, N_EPISODES + 1):#20.000 defa eÄŸitim epizotu baÅŸlatÄ±lÄ±r.
    # Rastgele bir eÄŸitim Ã¶rneÄŸi seÃ§ (durum/satÄ±r)  1. Rastgele eÄŸitim Ã¶rneÄŸi seÃ§mek
    idx = np.random.randint(len(X_train_disc))#EÄŸitim veri setinden rastgele bir hasta (durum) seÃ§ilir.
    s_disc = X_train_disc[idx]#seÃ§ilen durum alÄ±nÄ±r
    true_label = y_train.iloc[idx]#COVID-19'un gerÃ§ekte olup olmadÄ±ÄŸÄ±) alÄ±nÄ±r.
    
    # Durumu hash'e Ã§evir
    s_hash = state_to_index(s_disc)#SeÃ§ilen durum, $Q$-Tablosu'nda anahtar olarak kullanÄ±lacak benzersiz bir hash deÄŸerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r.
    
    # 7.1. Epsilon-greedy (KeÅŸif)  2. Epsilon-greedy ile rastgele eylem seÃ§mek (keÅŸif)
    if random.random() < eps:#eÄŸer rastgele Ã¼retilen sayÄ± mevcut eps deÄŸerinden kÃ¼Ã§Ã¼kse
        a_idx = random.randrange(n_actions) #ajan rastgele bir eylem seÃ§er
    else:
        #  Aksi takdirde, ajan Q-Tablosu'na bakar ve mevcut durumda en yÃ¼ksek beklenen Ã¶dÃ¼le sahip olan eylemi seÃ§erek Ã¶ÄŸrendiÄŸi bilgiyi sÃ¶mÃ¼rÃ¼r.
        a_idx = np.argmax(Q_table[s_hash]) 

    a = actions[a_idx] #daha Ã¶nce belirlenen eylem indeksini (a_idx), ajan tarafÄ±ndan gerÃ§ekleÅŸtirilecek gerÃ§ek eyleme (a) dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
    r = reward_fn(a, true_label) #ajanÄ±n amacÄ±nÄ± belirleyen kritik fonksiyondur.
    #Girdi 1 (a): AjanÄ±n tahmini sonucu veya en uygun gÃ¶rdÃ¼ÄŸÃ¼ eylemi yansÄ±tÄ±r.
    #Ã–rneÄŸin, ajan hastanÄ±n Ã¶zelliklerine bakarak COVID-19 olma ihtimalinin yÃ¼ksek olduÄŸunu tahmin eder ve buna dayanarak Tedavi Et (a=2) eylemini seÃ§er.
    #Girdi 2 (true_label): SeÃ§ilen hastanÄ±n gerÃ§ek COVID-19 durumu (Ã–rn: 1 - COVID var veya 0 - COVID yok).
    #Ã‡Ä±ktÄ± (r): AjanÄ±n bu eylem-sonuÃ§ Ã§ifti iÃ§in aldÄ±ÄŸÄ± sayÄ±sal deÄŸer (Ã¶dÃ¼l >0 veya ceza <0).
    
    # 7.2. Q-Tablosu GÃ¼ncellemesi (Bellman Denklemi)
  #Bu kod bloÄŸu, Q-Learning AlgoritmasÄ±nÄ±n Kalbini, yani Q-Tablosu'nu gÃ¼ncelleme iÅŸlemini gerÃ§ekleÅŸtirir. 
  #Bu gÃ¼ncelleme, ajanÄ±n deneyimlerinden Ã¶ÄŸrenmesini saÄŸlayan Bellman Denklemi'nin uygulanmasÄ±dÄ±r.Q-Tablosu GÃ¼ncelleme AdÄ±mlarÄ±
  #Bu kÄ±sÄ±m, ajanÄ±n bir eylem yaptÄ±ktan sonra kazandÄ±ÄŸÄ± Ã¶dÃ¼lÃ¼ kullanarak, mevcut durum-eylem Ã§ifti iÃ§in tahminini nasÄ±l dÃ¼zelttiÄŸini gÃ¶sterir.  
    # Sonraki durumu simÃ¼le et (yine rastgele bir sonraki durum Ã§ekiyoruz)
    idx_next = np.random.randint(len(X_train_disc))#EÄŸitim veri setinden rastgele bir hasta (durum) seÃ§ilir.
    s_next_disc = X_train_disc[idx_next]#seÃ§ilen durum alÄ±nÄ±r
    s_next_hash = state_to_index(s_next_disc)#SeÃ§ilen durum, Q-Tablosu'nda anahtar olarak kullanÄ±lacak benzersiz bir hash deÄŸerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r.

    # Sonraki durumun Q-Tablosundaki en yÃ¼ksek deÄŸerini bul
    Q_next_max = np.max(Q_table[s_next_hash])#Ajan, mevcut eyleminin faydasÄ±nÄ± deÄŸerlendirirken,
    #bu eylem sonucu geÃ§eceÄŸi yeni durumdan (s') gelecekte alabileceÄŸi en iyi potansiyel Ã¶dÃ¼lÃ¼ hesaba katmak zorundadÄ±r.

    # GÃ¼ncelleme iÃ§in Bellman Denklemi
    # Q(s, a) = Q(s, a) + ALPHA * [r + GAMMA * max(Q(s', a')) - Q(s, a)]
    old_q = Q_table[s_hash][a_idx]#Mevcut durum ($s$) ve eylem ($a$) iÃ§in ajanÄ±n eski tahminidir.
    new_q_value = old_q + ALPHA * (r + GAMMA * Q_next_max - old_q)#Ajan iÃ§in yeni bilgi (hedef) veya dÃ¼zeltilmiÅŸ Ã¶dÃ¼ldÃ¼r.
    
    # Q-Tablosunu gÃ¼ncelle
    Q_table[s_hash][a_idx] = new_q_value
    
    rewards_history.append(r)#AjanÄ±n o anki eyleminden dolayÄ± elde ettiÄŸi anlÄ±k Ã¶dÃ¼lÃ¼ (r),
    #eÄŸitim sÃ¼recindeki Ã¶dÃ¼llerin tÃ¼m geÃ§miÅŸini tutan rewards_history listesine ekler.
    eps = max(EPS_END, eps * EPS_DECAY)

    if ep % 2000 == 0:
        print(f"Epizot: {ep}/{N_EPISODES}, Epsilon: {eps:.4f}")

# ---------------- 8ï¸âƒ£ Test aÅŸamasÄ± ----------------
correct, total, cum_reward = 0, 0, 0
#correct: AjanÄ±n doÄŸru kabul edilen kararlarÄ±nÄ±n sayÄ±sÄ±nÄ± tutar.
#Ä°ÅŸlenen toplam test Ã¶rneÄŸi sayÄ±sÄ±nÄ± (hasta sayÄ±sÄ±nÄ±) tutar.
#Test aÅŸamasÄ± boyunca ajanÄ±n topladÄ±ÄŸÄ± kÃ¼mÃ¼latif (birikmiÅŸ) Ã¶dÃ¼lÃ¼ tutar.
print("\nğŸ”¬ Test aÅŸamasÄ± baÅŸlatÄ±ldÄ±...")
##DÃ¶ngÃ¼: Kod, test durumlarÄ± (X_test_disc) ve bu durumlarÄ±n gerÃ§ek etiketleri (y_test) Ã¼zerinde eÅŸ zamanlÄ± olarak dÃ¶ner.
for s_disc, true in zip(X_test_disc, y_test):#
    s_hash = state_to_index(s_disc)#Her bir test durumu, eÄŸitimde kullanÄ±lan state_to_index fonksiyonu ile hash deÄŸerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r.
#not:Q-Tablosu yalnÄ±zca eÄŸitim aÅŸamasÄ±nda ajanÄ±n karÅŸÄ±laÅŸtÄ±ÄŸÄ± benzersiz durumlarÄ± iÃ§erir.    
    # Q-Tablosunda durum varsa en iyi eylemi seÃ§, yoksa 0 (no action) seÃ§
    if s_hash in Q_table:
        a_idx = np.argmax(Q_table[s_hash])
        a = actions[a_idx]
    else:
        # Bu durum eÄŸitimde gÃ¶rÃ¼lmemiÅŸtir, varsayÄ±lan olarak "HiÃ§bir ÅŸey yapma"
        a = 0 
#EÄŸer durumun hash'i Q-Tablosu'nda varsa, ajan o durum iÃ§in Ã¶ÄŸrendiÄŸi Q-deÄŸerlerine bakar 
#ve np.argmax ile en yÃ¼ksek deÄŸere sahip olan eylemi (a) seÃ§er.
#EÄŸer durum eÄŸitimde hiÃ§ gÃ¶rÃ¼lmemiÅŸse (yani hash Q-Tablosu'nda yoksa), ajan risk almamak iÃ§in 
#varsayÄ±lan olarak "HiÃ§bir ÅŸey yapma" (a = 0) eylemini seÃ§er.
    r = reward_fn(a, true)#SeÃ§ilen eylem (a) ve hastanÄ±n gerÃ§ek durumu (true) kullanÄ±larak Ã–DÃœL HESAPLANIR
    
    cum_reward += r#Hesaplanan Ã¶dÃ¼l cum_reward'a eklenir. Bu, ajanÄ±n genel ekonomik baÅŸarÄ±sÄ±nÄ± Ã¶lÃ§er.
    total += 1#Bu, test veri setindeki iÅŸlenen toplam hasta (durum) sayÄ±sÄ±nÄ± bir artÄ±rÄ±r. 
    #Bu deÄŸiÅŸken, sonunda doÄŸruluk oranÄ±nÄ± (correct/total) hesaplamak iÃ§in kullanÄ±lÄ±r.
    
    # Metrik: EÄŸer COVID-19 ise test/tedavi (1, 2) yapÄ±ldÄ±ysa VEYA COVID-19 deÄŸilse hiÃ§bir ÅŸey yapma (0) yapÄ±ldÄ±ysa doÄŸru kabul et.
    if (true == 1 and a in [1, 2]) or (true == 0 and a == 0):
        correct += 1
#true == 1 and a in [1, 2]
#AnlamÄ±: Hastada gerÃ§ekte COVID-19 Varsa (true == 1), ajanÄ±n eylemi Test Et (1) veya Tedavi Et (2) olmalÄ±dÄ±r.
#true == 0 and a == 0
#Hastada gerÃ§ekte COVID-19 Yoksa (true == 0), ajanÄ±n eylemi HiÃ§bir ÅŸey yapma (0) olmalÄ±dÄ±r.


print(f"\nâœ… Test doÄŸruluk (oyuncak metrik): {correct/total:.3f}")
print(f"ğŸ Toplam Ã¶dÃ¼l: {cum_reward:.2f}")

# ---------------- 9ï¸âƒ£ Grafik ----------------
window_size = 500 # Daha uzun bir pencere ile hareketli ortalama
plt.figure(figsize=(12, 6))
plt.plot(np.convolve(rewards_history, np.ones(window_size)/window_size, mode='valid'))
plt.title("Q-Learning COVID: 500-adÄ±mlÄ±k Hareketli Ortalama Ã–dÃ¼l")
plt.xlabel("EÄŸitim AdÄ±mÄ±")
plt.ylabel("Ortalama Ã–dÃ¼l")
plt.show()