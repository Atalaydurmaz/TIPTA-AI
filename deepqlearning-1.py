# -*- coding: utf-8 -*-

import pandas as pd
import numpy as np
import random
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
# ESKÄ° SATIR (Hata Veren): from tensorflow.keras.optimizers.legacy import Adam
from tensorflow.keras.optimizers import Adam # âœ… Yeni ve Keras 3 ile uyumlu Adam importu
from sklearn.preprocessing import KBinsDiscretizer

# ---------------- 1ï¸âƒ£ Veri setini internetten Ã§ek ----------------
# Veri yolu gÃ¼ncellendi
print("ğŸ“¥ UCI COVID-19 verisi indiriliyor...")
# LÃ¼tfen KENDÄ° DOSYA YOLUNUZU BURAYA YAZIN
file_path = r"C://Users//Fatma//Desktop//V4//covid_50.csv" 
try:
    df = pd.read_csv(file_path)
    print("âœ… Veri baÅŸarÄ±yla yÃ¼klendi!")
except FileNotFoundError:
    print("HATA: Dosya yolu bulunamadÄ±. LÃ¼tfen dosya yolunu kontrol edin.")
    exit()

# ---------------- 2ï¸âƒ£ Ã–n iÅŸleme ----------------
target_col = 'COVID-19'

# Kategorikleri sayÄ±ya Ã§evir
for c in df.columns:
    if df[c].dtype == object:
        df[c] = LabelEncoder().fit_transform(df[c].astype(str))

X = df.drop(columns=[target_col])
y = df[target_col]

# Eksik veri doldur
X = X.fillna(0)
y = y.fillna(0)

# ---------------- 3ï¸âƒ£ Diskritizasyon ve State Temsili ----------------
# Q-Learning'deki aÅŸÄ±rÄ± bÃ¼yÃ¼k durum sayÄ±sÄ±nÄ± Ã§Ã¶zmek iÃ§in diskitizasyon seviyesi korunmuÅŸtur.
# Ancak DQN'de bu durumlar doÄŸrudan Sinir AÄŸÄ±na GÄ°RDÄ° olarak verilecektir, HASH KULLANILMAYACAKTIR.
N_BINS = 2 #â˜»o anda verideki sayÄ±sal Ã§eÅŸitliliÄŸi 2 ye ayÄ±rÄ±rsÄ±nÄ±z
kbd = KBinsDiscretizer(n_bins=N_BINS, encode='ordinal', strategy='uniform')
#KBinsDiscretizer, popÃ¼ler makine Ã¶ÄŸrenimi kÃ¼tÃ¼phanesi scikit-learn'den gelen bir araÃ§tÄ±r. AmacÄ±, veri kÃ¼mesindeki her bir sÃ¼rekli sayÄ±sal Ã¶zelliÄŸi alÄ±p, o deÄŸerleri belirli sayÄ±da eÅŸit parÃ§aya (kutucuÄŸa veya kategoriye) ayÄ±rmaktÄ±r.
X_disc_all = kbd.fit_transform(X) # BasitÃ§e: "BÃ¶leceÄŸim sÄ±nÄ±rlamalarÄ± Ã¶ÄŸren ve hemen tÃ¼m veriyi 0 veya 1 olarak kategorilendir."

# Diskritize edilmiÅŸ (ayrÄ±klaÅŸtÄ±rÄ±lmÄ±ÅŸ) veriyi kullanacaÄŸÄ±z, bu da durum uzayÄ±nÄ±n boyutunu azaltÄ±r.
X_train_disc, X_test_disc, y_train, y_test = train_test_split(
    X_disc_all, y, test_size=0.2, random_state=42
)

n_features = X_train_disc.shape[1]# Bu, modelin bir hastanÄ±n durumunu tanÄ±mlayan Ã¶zellik sayÄ±sÄ±dÄ±r (Ã¶rneÄŸin, $21$ semptom ve risk faktÃ¶rÃ¼). Sinir AÄŸÄ±nÄ±z bu $21$ Ã¶zelliÄŸi girdi olarak alacaktÄ±r.
actions = [0, 1, 2]  # 0: no action, 1: test, 2: treat
#AnlamÄ±: AjanÄ±n o anda alabileceÄŸi tÃ¼m muhtemel eylemleri (kararlarÄ±) tanÄ±mlar.AÃ§Ä±klama: 
#Bu, bir hastanÄ±n durumuna yanÄ±t olarak alÄ±nabilecek Ã¼Ã§ farklÄ± karardÄ±r:0: HiÃ§bir ÅŸey yapma1: COVID-19 testi yap2: Tedavi etRolÃ¼: 
#Bu liste, RL ajanÄ±nÄ±zÄ±n eylem uzayÄ±nÄ± oluÅŸturur. AjanÄ±n amacÄ±, her durumda bu eylemlerden en yÃ¼ksek $Q$ deÄŸerine sahip olanÄ± seÃ§mektir.
n_actions = len(actions)

print(f"\nModel Girdi Boyutu (Ã–zellik SayÄ±sÄ±): {n_features}")
print(f"Eylem sayÄ±sÄ±: {n_actions}")

# ---------------- 4ï¸âƒ£ Ã–dÃ¼l fonksiyonu ----------------
def reward_fn(action, true_label):
    if action == 0:  # hiÃ§bir ÅŸey yapma
        return 0 if true_label == 0 else -1
    if action == 1:  # test et
        return 1 if true_label == 1 else -0.2
    if action == 2:  # tedavi et
        return 1 if true_label == 1 else -1
    return 0

# ---------------- 5ï¸âƒ£ DQN Modeli (Q-Tablosu Yerine) ----------------
def build_dqn_model(input_shape, output_shape):
    model = Sequential()
    # Girdi katmanÄ±, Ã¶zellik sayÄ±sÄ± kadar nÃ¶ron alÄ±r
    model.add(Dense(32, activation='relu', input_shape=(input_shape,)))
    model.add(Dense(32, activation='relu'))
    # Ã‡Ä±ktÄ± katmanÄ±, her eylem iÃ§in bir Q deÄŸeri verir
    model.add(Dense(output_shape, activation='linear'))
    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))
    return model

# Q-DeÄŸerlerini tahmin edecek olan modelimiz
#Bu aÄŸ, bir durum (s) verildiÄŸinde, ajanÄ±n hangi eylemi seÃ§eceÄŸine karar vermek iÃ§in o eylemlerin gÃ¼ncel Q deÄŸerlerini tahmin eder.
#Ã¶ÄŸrenme sÃ¼recini gerÃ§ekleÅŸtiren asÄ±l sinir aÄŸÄ±dÄ±r.
model = build_dqn_model(n_features, n_actions)
# Hedef modeli, stabilizasyon iÃ§in kullanÄ±lÄ±r (DQN'in kilit noktasÄ±)
#ana model ile aynÄ± mimariye sahip olan, ancak farklÄ± bir amaca hizmet edeR
#Bu aÄŸ, Bellman Denklemi (Q-Ã¶ÄŸrenme kuralÄ±) kullanÄ±lÄ±rken hesaplanan hedef Q deÄŸerini tahmin eder
target_model = build_dqn_model(n_features, n_actions)
target_model.set_weights(model.get_weights())

# ---------------- 6ï¸âƒ£ DQN Parametreleri ----------------
ALPHA = 0.5  # RL Ã–ÄŸrenme OranÄ± (Åimdi Keras Ã¶ÄŸrenme oranÄ±nÄ± kullanacaÄŸÄ±z)
GAMMA = 0.9  # Ä°ndirgeme FaktÃ¶rÃ¼ (Q-Learning'de 0 idi, DQN'de 0.9 yaptÄ±k)
EPS_START = 1.0
EPS_END = 0.01 # Daha uzun keÅŸif iÃ§in biraz dÃ¼ÅŸÃ¼rÃ¼ldÃ¼
EPS_DECAY = 0.99999 # YavaÅŸ dÃ¼ÅŸÃ¼ÅŸ
N_EPISODES = 20000 # Tablo yerine aÄŸ eÄŸittiÄŸimiz iÃ§in daha fazla adÄ±m gerekir
BATCH_SIZE = 64
TARGET_UPDATE_FREQ = 100 # Hedef aÄŸÄ± kaÃ§ adÄ±mda bir gÃ¼ncelleyeceÄŸimiz

eps = EPS_START
rewards_history = []
print("\nğŸ¯ DQN baÅŸlatÄ±ldÄ±...")

# ---------------- 7ï¸âƒ£ EÄŸitim DÃ¶ngÃ¼sÃ¼ (DQN MantÄ±ÄŸÄ±) ----------------
# DQN, rastgele bir durumdan baÅŸlar ve her adÄ±mda aÄŸÄ± eÄŸitir
for ep in range(1, N_EPISODES + 1):
    # Rastgele bir eÄŸitim Ã¶rneÄŸi seÃ§ (durum/satÄ±r)
    idx = np.random.randint(len(X_train_disc))
    s = X_train_disc[idx]
    true_label = y_train.iloc[idx]
    
    # Durumu aÄŸa besle (bir sonraki Q deÄŸerlerini tahmin et)
    # Keras'a uygun olmasÄ± iÃ§in boyutu ayarla: (1, n_features)
    s_input = s.reshape(1, n_features) 
    
    # 7.1. Epsilon-greedy (KeÅŸif)
    if random.random() < eps:
        a_idx = random.randrange(n_actions)
    else:
        # Mevcut aÄŸdan Q deÄŸerlerini tahmin et ve en iyisini seÃ§
        q_values = model.predict(s_input, verbose=0)[0]
        a_idx = np.argmax(q_values)

    a = actions[a_idx]
    r = reward_fn(a, true_label)
    
    # 7.2. Q-DeÄŸeri GÃ¼ncellemesi (Hedef Hesaplama)
    
    # Mevcut Q deÄŸerlerini al (bu, aÄŸÄ±n Ã§Ä±ktÄ±sÄ±dÄ±r)
    current_q = model.predict(s_input, verbose=0)[0]
    
    # Sonraki durum (Bu problemde, her adÄ±m baÄŸÄ±msÄ±z bir durumdur. Sonraki durumu simÃ¼le etmek zor.)
    # BasitleÅŸtirilmiÅŸ yaklaÅŸÄ±mla: Sonraki durumun (s') deÄŸeri (Q_next) bu problemde sÄ±fÄ±r alÄ±nabilir (GAMMA=0'daki gibi).
    # Ancak DQN iÃ§in GAMMA'yÄ± 0.9 aldÄ±k. BasitÃ§e sonraki durumu aynÄ± veri setinden rastgele Ã§ekelim.
    idx_next = np.random.randint(len(X_train_disc))
    s_next = X_train_disc[idx_next]
    s_next_input = s_next.reshape(1, n_features)

    # Hedef aÄŸdan (target_model) sonraki Q deÄŸerini tahmin et (stabilizasyon iÃ§in)
    Q_next_all = target_model.predict(s_next_input, verbose=0)[0]
    Q_next_max = np.max(Q_next_all)

    # Yeni Q hedef deÄŸeri (Bellman Denklemi)
    new_q_target = r + GAMMA * Q_next_max
    
    # YENÄ° HEDEF vektÃ¶rÃ¼nÃ¼ oluÅŸtur (current_q'nun bir kopyasÄ±)
    target_f = current_q.copy()
    # Sadece seÃ§ilen eylemin Q deÄŸerini yeni hedef ile deÄŸiÅŸtir
    target_f[a_idx] = new_q_target
    
    # 7.3. AÄŸÄ± EÄŸitme
    # AÄŸ, mevcut durumu girdi olarak alÄ±p, gÃ¼ncellenmiÅŸ Q deÄŸerlerini (target_f) tahmin etmeye Ã§alÄ±ÅŸÄ±r.
    model.fit(s_input, target_f.reshape(1, n_actions), epochs=1, verbose=0)
    
    rewards_history.append(r)
    eps = max(EPS_END, eps * EPS_DECAY)

    # 7.4. Hedef AÄŸÄ± GÃ¼ncelle
    if ep % TARGET_UPDATE_FREQ == 0:
        target_model.set_weights(model.get_weights())
        
    if ep % 1000 == 0:
         print(f"Epizot: {ep}/{N_EPISODES}, Epsilon: {eps:.4f}")


# ---------------- 8ï¸âƒ£ Test aÅŸamasÄ± ----------------
correct, total, cum_reward = 0, 0, 0

print("\nğŸ”¬ Test aÅŸamasÄ± baÅŸlatÄ±ldÄ±...")

for s_disc, true in zip(X_test_disc, y_test):
    s_input = s_disc.reshape(1, n_features)
    
    # PolitikamÄ±z artÄ±k Q-tablosu deÄŸil, Sinir AÄŸÄ±dÄ±r
    q_values = model.predict(s_input, verbose=0)[0]
    
    a_idx = np.argmax(q_values)
    a = actions[a_idx]
    r = reward_fn(a, true)
    
    cum_reward += r
    total += 1
    
    # Metrik: EÄŸer COVID-19 ise test/tedavi (1, 2) yapÄ±ldÄ±ysa VEYA COVID-19 deÄŸilse hiÃ§bir ÅŸey yapma (0) yapÄ±ldÄ±ysa doÄŸru kabul et.
    if (true == 1 and a in [1, 2]) or (true == 0 and a == 0):
        correct += 1

print(f"\nâœ… Test doÄŸruluk (oyuncak metrik): {correct/total:.3f}")
print(f"ğŸ Toplam Ã¶dÃ¼l: {cum_reward:.2f}")

# ---------------- 9ï¸âƒ£ Grafik ----------------
window_size = 500 # Daha uzun bir pencere ile hareketli ortalama
plt.figure(figsize=(12, 6))
plt.plot(np.convolve(rewards_history, np.ones(window_size)/window_size, mode='valid'))
plt.title("DQN COVID: 500-adÄ±mlÄ±k Hareketli Ortalama Ã–dÃ¼l")
plt.xlabel("EÄŸitim AdÄ±mÄ±")
plt.ylabel("Ortalama Ã–dÃ¼l")
plt.show()